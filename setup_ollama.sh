#!/bin/bash

# NeuraLex Platform - Ollama Setup fÃ¼r Hetzner Server
# Installiert und konfiguriert Ollama mit geeigneten Modellen fÃ¼r Dokumentanalyse

set -e

echo "ğŸš€ NeuraLex Platform - Ollama Setup startet..."

# System Updates
echo "ğŸ“¦ System wird aktualisiert..."
apt update && apt upgrade -y

# Ollama installieren
echo "ğŸ¤– Ollama wird installiert..."
curl -fsSL https://ollama.ai/install.sh | sh

# Ollama Service starten
echo "ğŸ”§ Ollama Service wird gestartet..."
systemctl start ollama
systemctl enable ollama

# Warten bis Ollama bereit ist
echo "â³ Warte auf Ollama Service..."
sleep 10

# Modelle fÃ¼r Dokumentanalyse herunterladen
echo "ğŸ“¥ Lade optimale Modelle fÃ¼r Dokumentanalyse..."

# Llama 3.2 (3B) - Gut fÃ¼r Klassifizierung und strukturierte Extraktion
echo "ğŸ“‹ Lade Llama 3.2 (3B) fÃ¼r Dokumentklassifizierung..."
ollama pull llama3.2

# Mistral 7B - Exzellent fÃ¼r JSON-Extraktion
echo "ğŸ” Lade Mistral 7B fÃ¼r strukturierte Datenextraktion..."
ollama pull mistral

# Phi-3 Mini - Schnell und effizient fÃ¼r kleinere Aufgaben
echo "âš¡ Lade Phi-3 Mini fÃ¼r schnelle Verarbeitung..."
ollama pull phi3

# Gemma 2B - Sehr ressourcenschonend
echo "ğŸ’ Lade Gemma 2B als Fallback-Modell..."
ollama pull gemma:2b

# Environment-Variablen fÃ¼r NeuraLex setzen
echo "ğŸ”§ Konfiguriere Umgebungsvariablen..."

cat << 'EOF' >> /etc/environment
# NeuraLex Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2
OLLAMA_FALLBACK_MODEL=gemma:2b
EOF

# Systemd Service fÃ¼r optimale Performance
echo "âš™ï¸ Konfiguriere Ollama Service fÃ¼r optimale Performance..."

cat << 'EOF' > /etc/systemd/system/ollama.service
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="OLLAMA_HOST=0.0.0.0:11434"
Environment="OLLAMA_MODELS=/usr/share/ollama/.ollama/models"

[Install]
WantedBy=default.target
EOF

# Service neu laden und starten
systemctl daemon-reload
systemctl restart ollama

# Teste Ollama Installation
echo "ğŸ§ª Teste Ollama Installation..."
sleep 5

# Test mit einfacher Anfrage
TEST_RESPONSE=$(curl -s -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2",
    "prompt": "Klassifiziere dieses Dokument: RECHNUNG Nr. 2024-001",
    "stream": false,
    "options": {"temperature": 0.1}
  }' | jq -r '.response // "ERROR"')

if [[ "$TEST_RESPONSE" != "ERROR" && "$TEST_RESPONSE" != "" ]]; then
    echo "âœ… Ollama Installation erfolgreich!"
    echo "ğŸ“Š Test-Antwort: $TEST_RESPONSE"
else
    echo "âŒ Ollama Test fehlgeschlagen"
    exit 1
fi

# Status-Ãœbersicht
echo ""
echo "ğŸ‰ Setup abgeschlossen!"
echo "ğŸ“‹ Installierte Modelle:"
ollama list

echo ""
echo "ğŸ”§ Konfiguration:"
echo "   Ollama Host: http://localhost:11434"
echo "   PrimÃ¤res Modell: llama3.2"
echo "   Fallback Modell: gemma:2b"
echo ""
echo "ğŸš€ NeuraLex Platform kann jetzt OCR-Dokumente analysieren:"
echo "   â€¢ Dokumenttyp-Klassifizierung (INVOICE, CONTRACT, etc.)"
echo "   â€¢ Event-Type-Extraktion (PAYMENT_DUE, CONTRACT_SIGNED, etc.)"
echo "   â€¢ Strukturierte Datenextraktion (JSON-Format)"
echo ""
echo "ğŸ’¡ NÃ¼tzliche Befehle:"
echo "   systemctl status ollama    # Service Status"
echo "   ollama list               # VerfÃ¼gbare Modelle"
echo "   ollama ps                 # Laufende Modelle"

echo "âœ¨ Setup erfolgreich abgeschlossen!"